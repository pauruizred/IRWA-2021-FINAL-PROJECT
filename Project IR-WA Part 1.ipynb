{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_XCewivA_M"
   },
   "source": [
    "Pau Ruiz 217962\n",
    "<br> Paula SarrÃ  216886\n",
    "<br> Jordi Valsells 218862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0PnpkquvA_W"
   },
   "source": [
    "# Project IR-WA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGRcJtAvA_X"
   },
   "source": [
    "#### Load Python packages\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DGxDY7dovA_Y",
    "outputId": "571d78b4-e2a2-44b5-8863-a2ff49167575",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RiyD_L7dvA_c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdDgqXK9vA_d"
   },
   "source": [
    "## Part 1: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gbxrCVFvA_e"
   },
   "source": [
    "#### Load data into memory\n",
    "The dataset is stored in the TSV file, and it contains 500 Wikipedia articles (one article per line). For each article we have the document id, document title and document body separated by \"|\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hEFEA_3QvA_f",
    "outputId": "4f401af0-391c-4ec1-86d0-d888d737f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tweets: 2399\n"
     ]
    }
   ],
   "source": [
    "#Path of the document\n",
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweets_data_raw = json.loads(line)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "#Change str keys to int\n",
    "tweets_data = {int(key):value for key, value in tweets_data_raw.items()}\n",
    "\n",
    "# Print first tweet of the dict\n",
    "#print(tweets_data[0])\n",
    "\n",
    "#Total number of tweets\n",
    "print('\\nTotal number of tweets: {}'.format(len(tweets_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j11BV7xcvA_g",
    "outputId": "4868c8d7-3eed-44d2-e2a2-f4b01d71d913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "ðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n"
     ]
    }
   ],
   "source": [
    "#we print the first tweet before processing\n",
    "print(tweets_data[0]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SnQtQJfCvA_i"
   },
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    hashtags - a list of hashtags that the text contained\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    line = line.lower() ## Transform in lowercase\n",
    "    line = re.sub(r'http\\S+', '', line) ## remove links\n",
    "    line = re.sub(r'[^A-Za-z0-9#\\n ]+', '', line) ## remove special characters (not # yet)\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    \n",
    "    hashtags=[]\n",
    "    \n",
    "    for word in range(len(line)):\n",
    "        if line[word][0]=='#':\n",
    "            line[word]=line[word].replace('#', '') ## now remove # too\n",
    "            hashtags.append(line[word]) ## and add to the list of hashtags\n",
    "            \n",
    "    line = [word for word in line if word not in stop_words]  ##eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] ## perform stemming\n",
    "    \n",
    "    return line, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jSf3cZDlvA_j"
   },
   "outputs": [],
   "source": [
    "length=len(tweets_data)\n",
    "hashtags=[None]*(length+1)\n",
    "\n",
    "#we apply our function build terms to the tweets dataset and also save the hashtags in a list\n",
    "for key in range(length):\n",
    "    tweets_data[key]['full_text'],hashtags[key] = build_terms(tweets_data[key]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1FR5bagFvA_j",
    "outputId": "1d115fb9-23cf-4103-8675-5d61d93a9a62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['intern',\n",
       "  'day',\n",
       "  'disast',\n",
       "  'risk',\n",
       "  'reduct',\n",
       "  'openwho',\n",
       "  'launch',\n",
       "  'multiti',\n",
       "  'core',\n",
       "  'curriculum',\n",
       "  'help',\n",
       "  'equip',\n",
       "  'compet',\n",
       "  'need',\n",
       "  'work',\n",
       "  'within',\n",
       "  'public',\n",
       "  'health',\n",
       "  'emerg',\n",
       "  'respons',\n",
       "  'start',\n",
       "  'learn',\n",
       "  'today',\n",
       "  'amp',\n",
       "  'ready4respons'],\n",
       " ['openwho', 'ready4response'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we print the first tweet after processing and the list of hashtags that corresponds to it\n",
    "tweets_data[0]['full_text'],hashtags[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project IR-WA Part 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
