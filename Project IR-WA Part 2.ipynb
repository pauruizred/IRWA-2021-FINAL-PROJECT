{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_XCewivA_M"
   },
   "source": [
    "Pau Ruiz 217962\n",
    "<br> Paula SarrÃ  216886\n",
    "<br> Jordi Valsells 218862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0PnpkquvA_W"
   },
   "source": [
    "# Project IR-WA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGRcJtAvA_X"
   },
   "source": [
    "#### Load Python packages\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DGxDY7dovA_Y",
    "outputId": "571d78b4-e2a2-44b5-8863-a2ff49167575",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RiyD_L7dvA_c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdDgqXK9vA_d"
   },
   "source": [
    "## PART 1: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gbxrCVFvA_e"
   },
   "source": [
    "#### Load data into memory\n",
    "The dataset is stored in the TSV file, and it contains 500 Wikipedia articles (one article per line). For each article we have the document id, document title and document body separated by \"|\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hEFEA_3QvA_f",
    "outputId": "4f401af0-391c-4ec1-86d0-d888d737f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tweets: 2399\n"
     ]
    }
   ],
   "source": [
    "#Path of the document\n",
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweets_data_raw = json.loads(line)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "#Change str keys to int\n",
    "tweets_data = {int(key):value for key, value in tweets_data_raw.items()}\n",
    "\n",
    "# Print first tweet of the dict\n",
    "#print(tweets_data[0])\n",
    "\n",
    "#Total number of tweets\n",
    "print('\\nTotal number of tweets: {}'.format(len(tweets_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Wed Oct 13 09:15:58 +0000 2021',\n",
       " 'id': 1448215930178310144,\n",
       " 'id_str': '1448215930178310144',\n",
       " 'full_text': \"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\nðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\",\n",
       " 'truncated': False,\n",
       " 'display_text_range': [0, 274],\n",
       " 'entities': {'hashtags': [{'text': 'OpenWHO', 'indices': [52, 60]},\n",
       "   {'text': 'Ready4Response', 'indices': [232, 247]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': 'https://t.co/hBFFOF0xKL',\n",
       "    'expanded_url': 'https://bit.ly/3wCa0Dr',\n",
       "    'display_url': 'bit.ly/3wCa0Dr',\n",
       "    'indices': [251, 274]}],\n",
       "  'media': [{'id': 1448215398814560259,\n",
       "    'id_str': '1448215398814560259',\n",
       "    'indices': [275, 298],\n",
       "    'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'url': 'https://t.co/fgZY22RWuS',\n",
       "    'display_url': 'pic.twitter.com/fgZY22RWuS',\n",
       "    'expanded_url': 'https://twitter.com/WHO/status/1448215930178310144/video/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1920, 'h': 1080, 'resize': 'fit'},\n",
       "     'medium': {'w': 1200, 'h': 675, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 383, 'resize': 'fit'}}}]},\n",
       " 'extended_entities': {'media': [{'id': 1448215398814560259,\n",
       "    'id_str': '1448215398814560259',\n",
       "    'indices': [275, 298],\n",
       "    'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'url': 'https://t.co/fgZY22RWuS',\n",
       "    'display_url': 'pic.twitter.com/fgZY22RWuS',\n",
       "    'expanded_url': 'https://twitter.com/WHO/status/1448215930178310144/video/1',\n",
       "    'type': 'video',\n",
       "    'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1920, 'h': 1080, 'resize': 'fit'},\n",
       "     'medium': {'w': 1200, 'h': 675, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 383, 'resize': 'fit'}},\n",
       "    'video_info': {'aspect_ratio': [16, 9],\n",
       "     'duration_millis': 97639,\n",
       "     'variants': [{'bitrate': 256000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/480x270/izK3M-OCh-xYweXi.mp4?tag=12'},\n",
       "      {'bitrate': 832000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/640x360/deOwD7OuDaJ7uiHk.mp4?tag=12'},\n",
       "      {'bitrate': 2176000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/1280x720/aOPOcEVxPItrZ2RR.mp4?tag=12'},\n",
       "      {'content_type': 'application/x-mpegURL',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/pl/4_kPEePepwPbCe8k.m3u8?tag=12&container=fmp4'}]},\n",
       "    'additional_media_info': {'monetizable': False}}]},\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'in_reply_to_status_id': 1448208458604584960,\n",
       " 'in_reply_to_status_id_str': '1448208458604584960',\n",
       " 'in_reply_to_user_id': 14499829,\n",
       " 'in_reply_to_user_id_str': '14499829',\n",
       " 'in_reply_to_screen_name': 'WHO',\n",
       " 'user': {'id': 14499829,\n",
       "  'id_str': '14499829',\n",
       "  'name': 'World Health Organization (WHO)',\n",
       "  'screen_name': 'WHO',\n",
       "  'location': 'Geneva, Switzerland',\n",
       "  'description': 'We are the #UnitedNationsâ€™ health agency - #HealthForAll.\\nâ–¶ï¸ Always check our latest tweets on #COVID19 for updated advice/information.',\n",
       "  'url': 'https://t.co/wVulKuROWG',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/wVulKuROWG',\n",
       "      'expanded_url': 'http://www.who.int',\n",
       "      'display_url': 'who.int',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 9963586,\n",
       "  'friends_count': 1743,\n",
       "  'listed_count': 34215,\n",
       "  'created_at': 'Wed Apr 23 19:56:27 +0000 2008',\n",
       "  'favourites_count': 11879,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': True,\n",
       "  'statuses_count': 64983,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'D0ECF8',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/14499829/1610970935',\n",
       "  'profile_link_color': '0396DB',\n",
       "  'profile_sidebar_border_color': '8C8C8C',\n",
       "  'profile_sidebar_fill_color': 'D9D9D9',\n",
       "  'profile_text_color': '000000',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'regular',\n",
       "  'withheld_in_countries': []},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 16,\n",
       " 'favorite_count': 52,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print a tweet\n",
    "tweets_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SnQtQJfCvA_i"
   },
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    hashtags - a list of hashtags that the text contained\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    line = line.lower() ## Transform in lowercase\n",
    "    line = re.sub(r'http\\S+', '', line) ## remove links\n",
    "    line = re.sub(r'[^A-Za-z0-9#\\n ]+', '', line) ## remove special characters (not # yet)\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    \n",
    "    hashtags=[]\n",
    "    \n",
    "    for word in range(len(line)):\n",
    "        if line[word][0]=='#':\n",
    "            line[word]=line[word].replace('#', '') ## now remove # too\n",
    "            hashtags.append(line[word]) ## and add to the list of hashtags\n",
    "            \n",
    "    line = [word for word in line if word not in stop_words]  ##eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] ## perform stemming\n",
    "    \n",
    "    return line, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jSf3cZDlvA_j"
   },
   "outputs": [],
   "source": [
    "length=len(tweets_data)\n",
    "hashtags={}\n",
    "\n",
    "#we apply our function build terms to the tweets dataset and also save the hashtags in a list\n",
    "for key in range(length):\n",
    "    tweets_data[key]['full_text'], hashtags[tweets_data[key]['id']] = build_terms(tweets_data[key]['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets_data):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets \n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of tweets where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}\n",
    "    for tweet in tweets_data:  # Remember, lines contain all documents\n",
    "        tweet_id = int(tweets_data[tweet]['id'])\n",
    "        terms =  tweets_data[tweet]['full_text']#page_title + page_text\n",
    "        title = \" \".join(tweets_data[tweet]['full_text'])\n",
    "        title_index[tweet_id]=title\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index, title_index = create_index(tweets_data)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of tweets that contain any of the query terms. \n",
    "    So, we will get the list of tweets for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query, hashtags = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid19 flight\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 738 for the searched query:\n",
      "\n",
      "\n",
      " TWEET= covid19 shown health emerg disast affect entir commun especi weak health system vulner popul like migrant indigen peopl live fragil humanitarian condit - tweet_id: 1448208458604584960\n",
      "\n",
      " TWEET= rt opsom si est completament vacunado pued contraer covid19 importa si est vacunado si todava est esperando - tweet_id: 1448163383493136385\n",
      "\n",
      " TWEET= rt drtedro covid19 major impact peopl mentalhealth thank doctorja whophilippin dedic work - tweet_id: 1448031156348362754\n",
      "\n",
      " TWEET= rt drtedro must appreci role privat sector play covid19 respons develop vaccin short - tweet_id: 1448049654923468803\n",
      "\n",
      " TWEET= rt drtedro donat arent enough deliv vaccinequ end covid19 pandem need stronger leadership ramp - tweet_id: 1448031115428696064\n",
      "\n",
      " TWEET= live drtedro end covid19 pandem road inclus recoveri vaccinequ - tweet_id: 1447972516161490944\n",
      "\n",
      " TWEET= take climateactioncal countri must set ambiti commit sustain healthi amp green recoveri pandem who call assur sustain recoveri covid19 - tweet_id: 1447686744476995588\n",
      "\n",
      " TWEET= join us celebr worldchang legaci henrietta lack whose helacel enabl medic breakthrough hpv polio vaccin covid19 research join us amp lacksfamili honour legaci amp call healthequ around world - tweet_id: 1447644123868114950\n",
      "\n",
      " TWEET= rt drtedro today conven annual mentalhealth forum ask particip focu recogn covid19 pandem - tweet_id: 1447624950081273859\n",
      "\n",
      " TWEET= covid19 pandem onlin ictfacilit violenc women girl continu heighten dayofthegirl let commit make social media onlin space safer girl - tweet_id: 1447584363009425412\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"\\n TWEET= {} - tweet_id: {}\".format(title_index[d_id],d_id))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many most common words to print: 10\n",
      "\n",
      "OK. The 10 most common words are as follows\n",
      "\n",
      "amp :  1030\n",
      "drtedro :  979\n",
      "health :  774\n",
      "covid19 :  759\n",
      "rt :  549\n",
      "vaccin :  545\n",
      "countri :  395\n",
      "peopl :  353\n",
      "support :  271\n",
      "global :  254\n"
     ]
    }
   ],
   "source": [
    "def find_keywords():\n",
    "    \"\"\"\n",
    "    Find the most repeated words in the dataset.\n",
    "    \"\"\"\n",
    "    wordcount = {}\n",
    "    for key in range(length):\n",
    "        words = tweets_data[key]['full_text']\n",
    "        for word in words:\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 1\n",
    "            else:\n",
    "                wordcount[word] += 1\n",
    "    n_print = int(input(\"How many most common words to print: \"))\n",
    "    print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\n",
    "    word_counter = collections.Counter(wordcount)\n",
    "    for word, count in word_counter.most_common(n_print):\n",
    "        print(word, \": \", count)\n",
    "find_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets_data, num_tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets\n",
    "    num_documents -- total number of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in tweets (tweets in the same order as in the main index)\n",
    "    df = defaultdict(int)  #tweet frequencies of terms in the corpus\n",
    "    title_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweet in tweets_data:\n",
    "        \n",
    "        tweet_id = int(tweets_data[tweet]['id'])\n",
    "        terms =  tweets_data[tweet]['full_text']#page_title + page_text\n",
    "        title = \" \".join(tweets_data[tweet]['full_text'])\n",
    "        title_index[tweet_id]=title\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] +=1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_tweets/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 53.15 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_tweets = len(tweets_data)\n",
    "index, tf, df, idf, title_index = create_index_tfidf(tweets_data, num_tweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each tweet \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores_df = [x[0] for x in doc_scores]\n",
    "    docs_id_df = [x[1] for x in doc_scores]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    result_doc_scores = [x[0] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    \n",
    "    return result_docs, doc_scores_df, docs_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of tweets that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query, hashtags = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs, doc_scores_df, docs_id_df = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    \n",
    "    return ranked_docs, doc_scores_df, docs_id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframe to perform evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 887 for the searched query:\n",
      "\n",
      "tweet_id= 1418489081026154497 - tweet: mrna covid19 vaccin safe vaccin read\n",
      "tweet_id= 1432284372690866186 - tweet: covid19 vaccin could save life get vaccin soon turn\n",
      "tweet_id= 1426206173225787392 - tweet: rt whowpro encourag love one get vaccin covid19 help regist get vaccin site vaccin\n",
      "tweet_id= 1418545630494924805 - tweet: covid19 vaccin halal read\n",
      "tweet_id= 1416433609091653633 - tweet: covid19 vaccin covid19 vaccin 10 countri rest vaccinequ end pandem togeth worldemojiday\n",
      "tweet_id= 1408416636084707335 - tweet: get vaccin even covid19\n",
      "tweet_id= 1430183353638998018 - tweet: facilit effect vaccin deliveri person disabl health provid provid access covid19 vaccin info amp vaccin process ensur access vaccin site vaccinequ\n",
      "tweet_id= 1439194751538974723 - tweet: rt drtedro glad thank covid19 vaccin walk vaccin process omanconvent storag\n",
      "tweet_id= 1437865493118005248 - tweet: who global covid19 vaccin target 70 popul countri vaccin mid2022 vaccinequ\n",
      "tweet_id= 1435321728197177347 - tweet: blood clot receiv covid19 astrazeneca janssen vaccin rare get vaccin soon turn\n",
      "Insert your query:\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 233 for the searched query:\n",
      "\n",
      "tweet_id= 1418678505328451584 - tweet: covid19 pandem\n",
      "tweet_id= 1417639665012719618 - tweet: olymp drtedro inde covid19 pandem ask us mani question amp pandem test failingdrtedro tokyo2020 agoal4al\n",
      "tweet_id= 1425874774534860805 - tweet: sinc start covid19 pandem work member state scientif commun better understand pandem began better prepar next one\n",
      "tweet_id= 1420486399996878856 - tweet: pandem start amp end commun work prevent futur pandem must start local strengthen public health surveil system detect contain diseas sourc contddrmikeryan\n",
      "tweet_id= 1419671329880612867 - tweet: covid19 pandem help survey 105 countri last year show 46 report disrupt malaria diagnosi treatment full impact pandem malaria may known timedrtedro\n",
      "tweet_id= 1415648246349901825 - tweet: drtedro jensspahn actacceler togeth end covid19 pandem horizon must ambit must higher work togeth prevent detect respond rapidli futur outbreak pandem potentialdrtedro actogeth\n",
      "tweet_id= 1409429921064230912 - tweet: 5we must learn lesson covid19 pandem teach us must everyth prepar prevent detect respond rapidli futur epidem pandem base upon foundat univers health coverag amp strong primaryhealthcaredrtedro\n",
      "tweet_id= 1421870697094205445 - tweet: rt unicef guid breastfeed safe covid19 pandem worldbreastfeedingweek\n",
      "tweet_id= 1447972516161490944 - tweet: live drtedro end covid19 pandem road inclus recoveri vaccinequ\n",
      "tweet_id= 1433079645042212870 - tweet: media brief new hub pandem amp epidem intellig drtedro\n",
      "Insert your query:\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 683 for the searched query:\n",
      "\n",
      "tweet_id= 1447422126076669957 - tweet: rt pahowho mani barrier afford access prevent peopl access mental health care ne\n",
      "tweet_id= 1434425246061408259 - tweet: ensur equiti access health learn today shape health workforc tomorrow openwho student access qualiti cours free time anywher\n",
      "tweet_id= 1435065096514912262 - tweet: lack access health inform one main barrier exclud peopl disabl access amp receiv everyday healthcar servic healthforal\n",
      "tweet_id= 1409863614362787857 - tweet: lack adequ health facil health workforc shortag amp inappropri access health product compound complex procur mechan characterist typifi major sid healthcar access often fragment amp poor qualiti\n",
      "tweet_id= 1427987381609713669 - tweet: drtedro pahowho immedi need ensur sustain humanitarian access amp continu health servic across countri afghanistan focu ensur women amp girl access femal healthworkersdrtedro\n",
      "tweet_id= 1447266150233124865 - tweet: close 1 billion peopl mentalhealth disord despit magnitud mental ill health rel peopl around access qualiti mental health servic hear helen speak truth access mental health care essenti\n",
      "tweet_id= 1410605147340763137 - tweet: 2020 alon 2 billion peopl lack access safe water 36 billion peopl lack access basic toilet 23 billion peopl lack access basic hygien facil find stat mean us\n",
      "tweet_id= 1446475342659653633 - tweet: rt whoafghanistan access health afghan fall reach winter approach health cluster p\n",
      "tweet_id= 1410999807418814466 - tweet: part commit generationequ forum pledg improv access qualiti amp rightsbas famili plan support countri increas adolesc access amp use contracept dissemin updat guidelin safe abort\n",
      "tweet_id= 1430183353638998018 - tweet: facilit effect vaccin deliveri person disabl health provid provid access covid19 vaccin info amp vaccin process ensur access vaccin site vaccinequ\n",
      "Insert your query:\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 535 for the searched query:\n",
      "\n",
      "tweet_id= 1447624956615987206 - tweet: rt drtedro also need advanc effort support countri integr mentalhealth support school commun\n",
      "tweet_id= 1445433891297406981 - tweet: provid support deliv essenti packag highqual matern newborn servic technic guidanc support countri around\n",
      "tweet_id= 1419540893078626306 - tweet: drtedro open countri bahrain offic reflect who commit serv countri incom level provid support tailor need countri place global health architecturedrtedro healthforal whoimpact\n",
      "tweet_id= 1447421286871977987 - tweet: rt alissonbeck support fifacom worldmentalhealthday let support feel depress reachout\n",
      "tweet_id= 1421113125395484674 - tweet: drtedro 152 countri offic around world central support countri strengthen health system improv health populationsdrtedro whoimpact\n",
      "tweet_id= 1422849022381920260 - tweet: worldbreastfeedingweek hospit support mother breastfe promot infant formula bottl teat support breastfeed\n",
      "tweet_id= 1415647910138687488 - tweet: drtedro jensspahn actacceler committe call countri support who call vaccin least 10 popul everi countri end septemberdrtedro covid19 vaccinequ\n",
      "tweet_id= 1422642008372490244 - tweet: find sweden address declin breastfeed rate use data support polici protect promot support\n",
      "tweet_id= 1423570298599788551 - tweet: hospit support mother breastfe refer commun resourc breastfeed support work commun improv breastfeed servic let support breastfeed mother worldbreastfeedingweek\n",
      "tweet_id= 1435567734503391233 - tweet: rt drtedro remain commit work africanunion end covid19 pandem support african countri\n",
      "Insert your query:\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 433 for the searched query:\n",
      "\n",
      "tweet_id= 1433411723046776840 - tweet: support need peopl dementia primari health care specialist care communitybas servic rehabilit longterm care palli care info\n",
      "tweet_id= 1440294899740381194 - tweet: dementia inevit consequ age support need peopl w dementia primari health care specialist care communitybas servic rehabilit longterm care palli care worldalzheimersday\n",
      "tweet_id= 1440310329838170118 - tweet: carer famili peopl live dementia 6 practic tip reach support take care ensur continu care respond chang flexibl commun effect\n",
      "tweet_id= 1447273648318988291 - tweet: seriou gap mentalhealth care result chronic underinvest worldmentalhealthday call invest mental health care sebastian say access mental health care essenti cope covid19 pandem\n",
      "tweet_id= 1446862614139740165 - tweet: today world hospic palli care day 1 peopl need palliativecar receiv let leav one behind make palli care access amp afford here\n",
      "tweet_id= 1420766147964588034 - tweet: countri caribbean improv avail qualiti care critic care patient train empow health worker whoimpact uhcpartnership\n",
      "tweet_id= 1423231583830564868 - tweet: one harm health care yet everi minut 5 patient die unsaf care global patient safeti action plan 20212030 provid strateg direct elimin avoid harm health care amp improv patientsafeti\n",
      "tweet_id= 1446746020445102080 - tweet: today world hospic palli care day dyk 55 million peopl around need palliativecar 12 global need met access palli care servic human right health\n",
      "tweet_id= 1418240095908401156 - tweet: worldbrainday know although current cure dementia support caregiv improv live peopl care\n",
      "tweet_id= 1438859468268257280 - tweet: worldpatientsafetyday take care babi pregnanc attend appoint provid full medic histori ask question follow health care team recommend\n"
     ]
    }
   ],
   "source": [
    "our_queries=['covid19 vaccine', 'pandemic', 'health access', 'countries support', 'people care']\n",
    "\n",
    "queries_df = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    print(\"Insert your query:\\n\")\n",
    "    query = our_queries[i]\n",
    "    ranked_docs, doc_scores_df, docs_id_df = search_tf_idf(query, index)\n",
    "    \n",
    "    hashtag_found = [0]*len(docs_id_df)\n",
    "    \n",
    "    for word in query.split():\n",
    "        for doc in range(len(docs_id_df)):\n",
    "            for h in hashtags[docs_id_df[doc]]:\n",
    "                #print(h)\n",
    "                if h == word:\n",
    "                    hashtag_found[doc] = 1 #as it is binary\n",
    "    if i == 0:\n",
    "        queries_df = pd.DataFrame()\n",
    "        queries_df['doc_id'] = docs_id_df\n",
    "        queries_df['q_id'] = i\n",
    "        queries_df['predicted_relevance'] = doc_scores_df\n",
    "        queries_df['y_true'] = hashtag_found\n",
    "    else:\n",
    "        #print(\"here\")\n",
    "        query_df = pd.DataFrame()\n",
    "        query_df['doc_id'] = docs_id_df\n",
    "        query_df['q_id'] = i\n",
    "        query_df['predicted_relevance'] = doc_scores_df\n",
    "        query_df['y_true'] = hashtag_found\n",
    "        queries_df = pd.concat([queries_df, query_df], ignore_index=True)\n",
    "    \n",
    "    top = 10\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "    for d_id in ranked_docs[:top]:\n",
    "        print(\"tweet_id= {} - tweet: {}\".format(d_id, title_index[d_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>q_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1448208458604584960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1448163383493136385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1448049654923468803</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1448031156348362754</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1448031115428696064</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>1409489891797446656</td>\n",
       "      <td>4</td>\n",
       "      <td>0.834446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>1408450795570335747</td>\n",
       "      <td>4</td>\n",
       "      <td>1.152974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>1408450214139240452</td>\n",
       "      <td>4</td>\n",
       "      <td>1.274653</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>1408449858365710339</td>\n",
       "      <td>4</td>\n",
       "      <td>0.956125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>1408449282731122699</td>\n",
       "      <td>4</td>\n",
       "      <td>1.209217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2771 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc_id  q_id  predicted_relevance  y_true\n",
       "0     1448208458604584960     0             0.201941       1\n",
       "1     1448163383493136385     0             0.177700       0\n",
       "2     1448049654923468803     0             0.849441       1\n",
       "3     1448031156348362754     0             0.285646       1\n",
       "4     1448031115428696064     0             0.264472       1\n",
       "...                   ...   ...                  ...     ...\n",
       "2766  1409489891797446656     4             0.834446       0\n",
       "2767  1408450795570335747     4             1.152974       0\n",
       "2768  1408450214139240452     4             1.274653       0\n",
       "2769  1408449858365710339     4             0.956125       0\n",
       "2770  1408449282731122699     4             1.209217       0\n",
       "\n",
       "[2771 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = queries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_score, k=10):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    \n",
    "    '''    \n",
    "    order = np.argsort(y_score)\n",
    "    order = order[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    relevant = sum(y_true == 1)\n",
    "    precision = float(relevant)/k\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision@10 for query with q_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@10: 0.9\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>q_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1418489081026154497</td>\n",
       "      <td>0</td>\n",
       "      <td>1.815488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1432284372690866186</td>\n",
       "      <td>0</td>\n",
       "      <td>1.548168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1426206173225787392</td>\n",
       "      <td>0</td>\n",
       "      <td>1.536664</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1418545630494924805</td>\n",
       "      <td>0</td>\n",
       "      <td>1.531077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1408416636084707335</td>\n",
       "      <td>0</td>\n",
       "      <td>1.531077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1416433609091653633</td>\n",
       "      <td>0</td>\n",
       "      <td>1.531077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1430183353638998018</td>\n",
       "      <td>0</td>\n",
       "      <td>1.525690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1439194751538974723</td>\n",
       "      <td>0</td>\n",
       "      <td>1.424210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1437865493118005248</td>\n",
       "      <td>0</td>\n",
       "      <td>1.424210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1435321728197177347</td>\n",
       "      <td>0</td>\n",
       "      <td>1.372348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc_id  q_id  predicted_relevance  y_true\n",
       "520  1418489081026154497     0             1.815488       1\n",
       "253  1432284372690866186     0             1.548168       1\n",
       "334  1426206173225787392     0             1.536664       1\n",
       "515  1418545630494924805     0             1.531077       1\n",
       "730  1408416636084707335     0             1.531077       1\n",
       "556  1416433609091653633     0             1.531077       0\n",
       "268  1430183353638998018     0             1.525690       1\n",
       "158  1439194751538974723     0             1.424210       1\n",
       "167  1437865493118005248     0             1.424210       1\n",
       "214  1435321728197177347     0             1.372348       1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for query 0\n",
    "k=10\n",
    "\n",
    "current_query = 0\n",
    "current_query_res = search_results[search_results[\"q_id\"] == 0] \n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Precision for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@10: 0.9\n",
      "\n",
      "==> Precision@10: 0.0\n",
      "\n",
      "==> Precision@10: 0.1\n",
      "\n",
      "==> Precision@10: 0.0\n",
      "\n",
      "==> Precision@10: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "\n",
    "    current_query_res = search_results[search_results[\"q_id\"] == i] \n",
    "    temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "    k=10\n",
    "\n",
    "    print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision@k - AP@K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, y_score, k=10):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    '''\n",
    "    \n",
    "    gtp = sum(y_true==1)\n",
    "    order = np.argsort(y_score)\n",
    "    order = order[::-1]\n",
    "    y_true = np.take(y_true, order[:k])            \n",
    "\n",
    "    ## if all docs are not relevant\n",
    "    if gtp==0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i+1)\n",
    "    \n",
    "    avg_precision_at_k = prec_at_i / gtp\n",
    "    \n",
    "    return avg_precision_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average precision @ 10 for query with q_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision at 10 is 0.906041\n",
      "Average precision at 10 is 0.911287\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "#compute avg precision\n",
    "current_query_res = search_results[search_results[\"q_id\"] == 0] \n",
    "\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "avg_precision = avg_precision_at_k(np.array(temp[\"y_true\"]), np.array(temp[\"predicted_relevance\"]), k)\n",
    "print(\"Average precision at %d is %4f\"%(k, avg_precision))\n",
    "\n",
    "# Check with average_precision_score of sklearn\n",
    "from sklearn.metrics import average_precision_score\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "avg_precision_sklearn = average_precision_score(np.array(temp[\"y_true\"]), np.array(temp[\"predicted_relevance\"][:k]))\n",
    "print(\"Average precision at %d is %4f\"%(k, avg_precision_sklearn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we obtain almost the same result, so we can say that we have implemented the average precision correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        q_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        y_true: actual score of the document for the query (ground truth).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @k : float\n",
    "    '''\n",
    "    avp = []\n",
    "    for q in search_res[\"q_id\"].unique(): #loop over all query id\n",
    "        curr_data = search_res[search_res[\"q_id\"] == q]  # select data for current query\n",
    "        temp = curr_data.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "        avp.append(avg_precision_at_k(np.array(temp[\"y_true\"]), np.array(temp[\"predicted_relevance\"]), k)) #append average precision for current query\n",
    "    \n",
    "    map_k = np.sum(avp) / len(avp)\n",
    "    \n",
    "    return map_k, avp # return mean average precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAP@10 for all queries of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20620811287477953"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_k, avp = map_at_k(search_results, 10)\n",
    "map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average precision for all queries to check result of MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision of query 0 0.9060405643738976\n",
      "Average precision of query 1 0\n",
      "Average precision of query 2 0.125\n",
      "Average precision of query 3 0\n",
      "Average precision of query 4 0\n"
     ]
    }
   ],
   "source": [
    "for q in range(0,5): #loop over all query id\n",
    "        curr_data = search_results[search_results[\"q_id\"] == q]  # select data for current query\n",
    "        temp = curr_data.sort_values(\"predicted_relevance\", ascending=False).head(10)\n",
    "        print('Average precision of query', q, avg_precision_at_k(np.array(temp[\"y_true\"]), np.array(temp[\"predicted_relevance\"]), 10)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(y_true, y_score, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    '''\n",
    "\n",
    "    order = np.argsort(y_score)\n",
    "    order = order[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(y_true) == 0: # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    \n",
    "    mrr = 1 / (np.argmax(y_true == 1) + 1)\n",
    "    \n",
    "    return mrr # hint: to get the position of the first relevant document use \"np.argmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRR@K for k=3, 5 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.2, 5: 0.2, 10: 0.2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr = {}\n",
    "for k in [3,5,10]:\n",
    "    RRs = []\n",
    "    for q in search_results['q_id'].unique(): # loop over all query ids\n",
    "        labels = np.array(search_results[search_results['q_id']==q]['y_true']) # get labels for current query\n",
    "        scores = np.array(search_results[search_results['q_id']==q]['predicted_relevance']) # get predicted score for current query\n",
    "        scores_sorted = -np.sort(-scores) #sorting the scores with a numpy array\n",
    "        RRs.append(rr_at_k(labels, scores_sorted, k)) # append RR for current query\n",
    "    mrr[k] = np.round(float(sum(RRs) / len(RRs)),4) # Mean RR at current k\n",
    "mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple levels of relevance metrics\n",
    "\n",
    "### NDCG - Normalized Discounted Cumulative Gain\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(y_true, y_score,  k=10):\n",
    "    order = np.argsort(y_score)\n",
    "    order = order[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = (2 ** y_true) - 1 # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2) # Compute denominator\n",
    "    return np.sum(gain / discounts) #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=10):    \n",
    "    dcg_max = dcg_at_k(y_true, y_true,  k) # Ideal dcg\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(y_true, y_score,  k) / dcg_max, 4)  # return ndcg@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the ð‘ð·ð¶ðº@10 for query with q_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@10 for query with q_id=0: 0.8611\n",
      "ndcg@10 for query with q_id=1: 0\n",
      "ndcg@10 for query with q_id=2: 0.0\n",
      "ndcg@10 for query with q_id=3: 0\n",
      "ndcg@10 for query with q_id=4: 0\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "\n",
    "for q_id in range(0,5):\n",
    "\n",
    "    labels = np.array(search_results[search_results['q_id'] == q_id][\"y_true\"])\n",
    "    scores = np.array(search_results[search_results['q_id'] == q_id][\"predicted_relevance\"])\n",
    "    scores_sorted = -np.sort(-scores) #sorting the scores with a numpy array\n",
    "    ndcg_k = np.round(ndcg_at_k(labels, scores_sorted, k),4)\n",
    "    print(\"ndcg@{} for query with q_id={}: {}\".format(k,q_id,ndcg_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average ð‘ð·ð¶ðº@10 (considering all queries/rankings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ndcg@10: 0.1722\n"
     ]
    }
   ],
   "source": [
    "ndcgs = []\n",
    "k=10\n",
    "for q in search_results['q_id'].unique(): # loop over all query ids\n",
    "    labels = np.array(search_results[search_results['q_id'] == q][\"y_true\"]) ## get labels for current query\n",
    "    scores = np.array(search_results[search_results['q_id'] == q][\"predicted_relevance\"]) # get predicted score for current query\n",
    "    scores_sorted = -np.sort(-scores) #sorting the scores with a numpy array\n",
    "    ndcgs.append(np.round(ndcg_at_k(labels, scores_sorted, k), 4)) # append NDCG for current query\n",
    "\n",
    "avg_ndcg = np.round(float(sum(ndcgs) / len(ndcgs)),4) # Compute average NDCG\n",
    "print(\"Average ndcg@{}: {}\".format(k,avg_ndcg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for key in range(length):\n",
    "    all_words.append(tweets_data[key][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12ec1626820>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model= Word2Vec(all_words, workers=4, vector_size=100, min_count=1, window=5)\n",
    "X = model.wv[model.wv.index_to_key]\n",
    "model_2 = TSNE(n_components=2, random_state=0)\n",
    "X_2 = model_2.fit_transform(X)\n",
    "x=X_2[:,0]\n",
    "y=X_2[:,1]\n",
    "plt.scatter(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
