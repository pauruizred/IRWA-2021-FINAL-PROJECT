{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_XCewivA_M"
   },
   "source": [
    "Pau Ruiz 217962\n",
    "<br> Paula SarrÃ  216886\n",
    "<br> Jordi Valsells 218862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0PnpkquvA_W"
   },
   "source": [
    "# Project IR-WA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGRcJtAvA_X"
   },
   "source": [
    "#### Load Python packages\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DGxDY7dovA_Y",
    "outputId": "571d78b4-e2a2-44b5-8863-a2ff49167575",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RiyD_L7dvA_c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdDgqXK9vA_d"
   },
   "source": [
    "## PART 1: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gbxrCVFvA_e"
   },
   "source": [
    "#### Load data into memory\n",
    "The dataset is stored in the TSV file, and it contains 500 Wikipedia articles (one article per line). For each article we have the document id, document title and document body separated by \"|\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hEFEA_3QvA_f",
    "outputId": "4f401af0-391c-4ec1-86d0-d888d737f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tweets: 2399\n"
     ]
    }
   ],
   "source": [
    "#Path of the document\n",
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweets_data_raw = json.loads(line)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "#Change str keys to int\n",
    "tweets_data = {int(key):value for key, value in tweets_data_raw.items()}\n",
    "\n",
    "# Print first tweet of the dict\n",
    "#print(tweets_data[0])\n",
    "\n",
    "#Total number of tweets\n",
    "print('\\nTotal number of tweets: {}'.format(len(tweets_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Wed Oct 13 09:15:58 +0000 2021',\n",
       " 'id': 1448215930178310144,\n",
       " 'id_str': '1448215930178310144',\n",
       " 'full_text': \"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\nðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\",\n",
       " 'truncated': False,\n",
       " 'display_text_range': [0, 274],\n",
       " 'entities': {'hashtags': [{'text': 'OpenWHO', 'indices': [52, 60]},\n",
       "   {'text': 'Ready4Response', 'indices': [232, 247]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': 'https://t.co/hBFFOF0xKL',\n",
       "    'expanded_url': 'https://bit.ly/3wCa0Dr',\n",
       "    'display_url': 'bit.ly/3wCa0Dr',\n",
       "    'indices': [251, 274]}],\n",
       "  'media': [{'id': 1448215398814560259,\n",
       "    'id_str': '1448215398814560259',\n",
       "    'indices': [275, 298],\n",
       "    'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'url': 'https://t.co/fgZY22RWuS',\n",
       "    'display_url': 'pic.twitter.com/fgZY22RWuS',\n",
       "    'expanded_url': 'https://twitter.com/WHO/status/1448215930178310144/video/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1920, 'h': 1080, 'resize': 'fit'},\n",
       "     'medium': {'w': 1200, 'h': 675, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 383, 'resize': 'fit'}}}]},\n",
       " 'extended_entities': {'media': [{'id': 1448215398814560259,\n",
       "    'id_str': '1448215398814560259',\n",
       "    'indices': [275, 298],\n",
       "    'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'url': 'https://t.co/fgZY22RWuS',\n",
       "    'display_url': 'pic.twitter.com/fgZY22RWuS',\n",
       "    'expanded_url': 'https://twitter.com/WHO/status/1448215930178310144/video/1',\n",
       "    'type': 'video',\n",
       "    'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1920, 'h': 1080, 'resize': 'fit'},\n",
       "     'medium': {'w': 1200, 'h': 675, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 383, 'resize': 'fit'}},\n",
       "    'video_info': {'aspect_ratio': [16, 9],\n",
       "     'duration_millis': 97639,\n",
       "     'variants': [{'bitrate': 256000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/480x270/izK3M-OCh-xYweXi.mp4?tag=12'},\n",
       "      {'bitrate': 832000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/640x360/deOwD7OuDaJ7uiHk.mp4?tag=12'},\n",
       "      {'bitrate': 2176000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/1280x720/aOPOcEVxPItrZ2RR.mp4?tag=12'},\n",
       "      {'content_type': 'application/x-mpegURL',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/pl/4_kPEePepwPbCe8k.m3u8?tag=12&container=fmp4'}]},\n",
       "    'additional_media_info': {'monetizable': False}}]},\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'in_reply_to_status_id': 1448208458604584960,\n",
       " 'in_reply_to_status_id_str': '1448208458604584960',\n",
       " 'in_reply_to_user_id': 14499829,\n",
       " 'in_reply_to_user_id_str': '14499829',\n",
       " 'in_reply_to_screen_name': 'WHO',\n",
       " 'user': {'id': 14499829,\n",
       "  'id_str': '14499829',\n",
       "  'name': 'World Health Organization (WHO)',\n",
       "  'screen_name': 'WHO',\n",
       "  'location': 'Geneva, Switzerland',\n",
       "  'description': 'We are the #UnitedNationsâ€™ health agency - #HealthForAll.\\nâ–¶ï¸ Always check our latest tweets on #COVID19 for updated advice/information.',\n",
       "  'url': 'https://t.co/wVulKuROWG',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/wVulKuROWG',\n",
       "      'expanded_url': 'http://www.who.int',\n",
       "      'display_url': 'who.int',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 9963586,\n",
       "  'friends_count': 1743,\n",
       "  'listed_count': 34215,\n",
       "  'created_at': 'Wed Apr 23 19:56:27 +0000 2008',\n",
       "  'favourites_count': 11879,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': True,\n",
       "  'statuses_count': 64983,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'D0ECF8',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/14499829/1610970935',\n",
       "  'profile_link_color': '0396DB',\n",
       "  'profile_sidebar_border_color': '8C8C8C',\n",
       "  'profile_sidebar_fill_color': 'D9D9D9',\n",
       "  'profile_text_color': '000000',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'regular',\n",
       "  'withheld_in_countries': []},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 16,\n",
       " 'favorite_count': 52,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print a tweet\n",
    "tweets_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SnQtQJfCvA_i"
   },
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    hashtags - a list of hashtags that the text contained\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    line = line.lower() ## Transform in lowercase\n",
    "    line = re.sub(r'http\\S+', '', line) ## remove links\n",
    "    line = re.sub(r'[^A-Za-z0-9#\\n ]+', '', line) ## remove special characters (not # yet)\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    \n",
    "    hashtags=[]\n",
    "    \n",
    "    for word in range(len(line)):\n",
    "        if line[word][0]=='#':\n",
    "            line[word]=line[word].replace('#', '') ## now remove # too\n",
    "            hashtags.append(line[word]) ## and add to the list of hashtags\n",
    "            \n",
    "    line = [word for word in line if word not in stop_words]  ##eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] ## perform stemming\n",
    "    \n",
    "    return line, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jSf3cZDlvA_j"
   },
   "outputs": [],
   "source": [
    "length=len(tweets_data)\n",
    "hashtags={}\n",
    "\n",
    "#we apply our function build terms to the tweets dataset and also save the hashtags in a list\n",
    "for key in range(length):\n",
    "    tweets_data[key]['full_text'], hashtags[tweets_data[key]['id']] = build_terms(tweets_data[key]['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets_data):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets \n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of tweets where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}\n",
    "    for tweet in tweets_data:  # Remember, lines contain all documents\n",
    "        tweet_id = int(tweets_data[tweet]['id'])\n",
    "        terms =  tweets_data[tweet]['full_text']#page_title + page_text\n",
    "        title = \" \".join(tweets_data[tweet]['full_text'])\n",
    "        title_index[tweet_id]=title\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index, title_index = create_index(tweets_data)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of tweets that contain any of the query terms. \n",
    "    So, we will get the list of tweets for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query, hashtags = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid vaccine\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 436 for the searched query:\n",
      "\n",
      "\n",
      " TWEET= rt whoafro congratul algeria algeria 16th countri africa reach mileston fulli vaccin 10 pop - tweet_id: 1448163447678676992\n",
      "\n",
      " TWEET= clinic case definit post covid19 condit also call long covid - tweet_id: 1446089970758860801\n",
      "\n",
      " TWEET= rt whophilippin vaccin cant stop covid19 alon help protect love one covid - tweet_id: 1447421491428143106\n",
      "\n",
      " TWEET= learn first malaria vaccin receiv vaccin challeng roll malaria vaccin midst pandem dr palonsomalaria explain sciencein5 week - tweet_id: 1446601142373425153\n",
      "\n",
      " TWEET= rt drtedro thank much govern peopl thailand solidar continu financi support covid - tweet_id: 1442947527716704260\n",
      "\n",
      " TWEET= recov covid19 still experienc certain symptom could post covid19 condit long covid symptom long last treatment option dr diazjv explain sciencein5 - tweet_id: 1422159909957869572\n",
      "\n",
      " TWEET= rt drtedro must appreci role privat sector play covid19 respons develop vaccin short - tweet_id: 1448049654923468803\n",
      "\n",
      " TWEET= support need fight covid19 pandem suppress transmiss reduc exposur mortal amp morbid counter misinform protect vulner acceler access tool includ vaccin info - tweet_id: 1446521938755694592\n",
      "\n",
      " TWEET= inform find socialmedia true use spread mislead news content protect friend fals inform covid19 vaccin checkbeforeyoushar - tweet_id: 1446220258591789056\n",
      "\n",
      " TWEET= join us celebr worldchang legaci henrietta lack whose helacel enabl medic breakthrough hpv polio vaccin covid19 research join us amp lacksfamili honour legaci amp call healthequ around world - tweet_id: 1447644123868114950\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"\\n TWEET= {} - tweet_id: {}\".format(title_index[d_id],d_id))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many most common words to print: 10\n",
      "\n",
      "OK. The 10 most common words are as follows\n",
      "\n",
      "amp :  1030\n",
      "drtedro :  979\n",
      "health :  774\n",
      "covid19 :  759\n",
      "rt :  549\n",
      "vaccin :  545\n",
      "countri :  395\n",
      "peopl :  353\n",
      "support :  271\n",
      "global :  254\n"
     ]
    }
   ],
   "source": [
    "def find_keywords():\n",
    "    \"\"\"\n",
    "    Find the most repeated words in the dataset.\n",
    "    \"\"\"\n",
    "    wordcount = {}\n",
    "    for key in range(length):\n",
    "        words = tweets_data[key]['full_text']\n",
    "        for word in words:\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 1\n",
    "            else:\n",
    "                wordcount[word] += 1\n",
    "    n_print = int(input(\"How many most common words to print: \"))\n",
    "    print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\n",
    "    word_counter = collections.Counter(wordcount)\n",
    "    for word, count in word_counter.most_common(n_print):\n",
    "        print(word, \": \", count)\n",
    "find_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets_data, num_tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets\n",
    "    num_documents -- total number of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in tweets (tweets in the same order as in the main index)\n",
    "    df = defaultdict(int)  #tweet frequencies of terms in the corpus\n",
    "    title_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweet in tweets_data:\n",
    "        \n",
    "        tweet_id = int(tweets_data[tweet]['id'])\n",
    "        terms =  tweets_data[tweet]['full_text']#page_title + page_text\n",
    "        title = \" \".join(tweets_data[tweet]['full_text'])\n",
    "        title_index[tweet_id]=title\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] +=1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_tweets/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 67.39 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_tweets = len(tweets_data)\n",
    "index, tf, df, idf, title_index = create_index_tfidf(tweets_data, num_tweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each tweet \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores_df = [x[0] for x in doc_scores]\n",
    "    docs_id_df = [x[1] for x in doc_scores]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    result_doc_scores = [x[0] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    \n",
    "    return result_docs, doc_scores_df, docs_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of tweets that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query, hashtags = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs, doc_scores_df, docs_id_df = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    \n",
    "    return ranked_docs, doc_scores_df, docs_id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning ranking with TF-IDF + cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " covid19 vaccine\n",
      "======================\n",
      "Top 20 results out of 887 for the searched query:\n",
      "\n",
      "tweet_id= 1418489081026154497 - tweet: mrna covid19 vaccin safe vaccin read\n",
      "tweet_id= 1432284372690866186 - tweet: covid19 vaccin could save life get vaccin soon turn\n",
      "tweet_id= 1426206173225787392 - tweet: rt whowpro encourag love one get vaccin covid19 help regist get vaccin site vaccin\n",
      "tweet_id= 1418545630494924805 - tweet: covid19 vaccin halal read\n",
      "tweet_id= 1416433609091653633 - tweet: covid19 vaccin covid19 vaccin 10 countri rest vaccinequ end pandem togeth worldemojiday\n",
      "tweet_id= 1408416636084707335 - tweet: get vaccin even covid19\n",
      "tweet_id= 1430183353638998018 - tweet: facilit effect vaccin deliveri person disabl health provid provid access covid19 vaccin info amp vaccin process ensur access vaccin site vaccinequ\n",
      "tweet_id= 1439194751538974723 - tweet: rt drtedro glad thank covid19 vaccin walk vaccin process omanconvent storag\n",
      "tweet_id= 1437865493118005248 - tweet: who global covid19 vaccin target 70 popul countri vaccin mid2022 vaccinequ\n",
      "tweet_id= 1435321728197177347 - tweet: blood clot receiv covid19 astrazeneca janssen vaccin rare get vaccin soon turn\n",
      "tweet_id= 1424628205365997568 - tweet: live hiv covid19 risk priorit vaccin vaccin safe find latest episod sciencein5\n",
      "tweet_id= 1418480188682211330 - tweet: ingredi covid19 vaccin safe read\n",
      "tweet_id= 1410270080873598979 - tweet: qampa askwho covid19 vaccin effect\n",
      "tweet_id= 1446172477130948608 - tweet: vaccinequ requir vaccin 70 global popul suffici vaccin suppli perspect achiev global covid19 vaccin target need equit distribut dose around\n",
      "tweet_id= 1434790971632336906 - tweet: covid19 variant amp vaccin covid19 vaccin provid strong protect seriou ill amp death get necessari dose develop maximum protect continu practic protect behaviour even vaccin stop covid19 variant\n",
      "tweet_id= 1429469791857823750 - tweet: covid19 still need vaccin long wait recoveri get vaccin know natur immun vs vaccin induc immun who chief scientist doctorsoumya explain sciencein5 week\n",
      "tweet_id= 1446112901681455116 - tweet: call countri updat nation covid19 vaccin target plan guid manufactur invest vaccin distributiondrtedro vaccinequ\n",
      "tweet_id= 1443834915410358292 - tweet: rt drtedro miss 10 covid19 vaccin target global failur must spur us vaccin 40\n",
      "tweet_id= 1426786068192366592 - tweet: get vaccin covid19 help protect get sick soon turn take vaccin approv covid19 vaccin thoroughli test provid high degre protect get serious ill amp die diseas\n",
      "tweet_id= 1423591981456838656 - tweet: get vaccin help protect around covid19\n",
      "\n",
      " pandemic\n",
      "======================\n",
      "Top 20 results out of 233 for the searched query:\n",
      "\n",
      "tweet_id= 1418678505328451584 - tweet: covid19 pandem\n",
      "tweet_id= 1417639665012719618 - tweet: olymp drtedro inde covid19 pandem ask us mani question amp pandem test failingdrtedro tokyo2020 agoal4al\n",
      "tweet_id= 1425874774534860805 - tweet: sinc start covid19 pandem work member state scientif commun better understand pandem began better prepar next one\n",
      "tweet_id= 1420486399996878856 - tweet: pandem start amp end commun work prevent futur pandem must start local strengthen public health surveil system detect contain diseas sourc contddrmikeryan\n",
      "tweet_id= 1419671329880612867 - tweet: covid19 pandem help survey 105 countri last year show 46 report disrupt malaria diagnosi treatment full impact pandem malaria may known timedrtedro\n",
      "tweet_id= 1415648246349901825 - tweet: drtedro jensspahn actacceler togeth end covid19 pandem horizon must ambit must higher work togeth prevent detect respond rapidli futur outbreak pandem potentialdrtedro actogeth\n",
      "tweet_id= 1409429921064230912 - tweet: 5we must learn lesson covid19 pandem teach us must everyth prepar prevent detect respond rapidli futur epidem pandem base upon foundat univers health coverag amp strong primaryhealthcaredrtedro\n",
      "tweet_id= 1421870697094205445 - tweet: rt unicef guid breastfeed safe covid19 pandem worldbreastfeedingweek\n",
      "tweet_id= 1447972516161490944 - tweet: live drtedro end covid19 pandem road inclus recoveri vaccinequ\n",
      "tweet_id= 1433079645042212870 - tweet: media brief new hub pandem amp epidem intellig drtedro\n",
      "tweet_id= 1433059041606049795 - tweet: hub pandem epidem intellig offici inaugur drtedro chancellor merkel\n",
      "tweet_id= 1434464365768429576 - tweet: plan quittobacco pandem risk covid19 pose tobacco use learn tobacco industri lure peopl consum tobacco pandem dr hebegouda explain health benefit quit tobacco sciencein5\n",
      "tweet_id= 1437741809753538561 - tweet: drmikeryan explain criteria call pandem public health emerg intern concern\n",
      "tweet_id= 1420487120221884417 - tweet: earli stage pandem amp partner establish covid19 suppli chain systemdrmikeryan\n",
      "tweet_id= 1416802026177155074 - tweet: live diabet risk covid19 pose stay safe healthi pandem sciencein5\n",
      "tweet_id= 1433070724781154310 - tweet: drtedro covid19 pandem defin crisi time taught world mani pain lessonsdrtedro\n",
      "tweet_id= 1433070593721749506 - tweet: drtedro lesson west africa suffici prepar world global pandem respiratori pathogendrtedro\n",
      "tweet_id= 1430107807441956880 - tweet: 4 seek support idea treati intern instrument pandem prepared responsedrtedro rc71afro\n",
      "tweet_id= 1422910758648745987 - tweet: drtedro g20org understat say cours covid19 pandem depend leadership g20orgdrtedro vaccinequ\n",
      "tweet_id= 1447226874262872067 - tweet: rt drtedro today worldmentalhealthday covid19 pandem major impact peopl mental health significantli\n",
      "\n",
      " health access\n",
      "======================\n",
      "Top 20 results out of 683 for the searched query:\n",
      "\n",
      "tweet_id= 1447422126076669957 - tweet: rt pahowho mani barrier afford access prevent peopl access mental health care ne\n",
      "tweet_id= 1434425246061408259 - tweet: ensur equiti access health learn today shape health workforc tomorrow openwho student access qualiti cours free time anywher\n",
      "tweet_id= 1435065096514912262 - tweet: lack access health inform one main barrier exclud peopl disabl access amp receiv everyday healthcar servic healthforal\n",
      "tweet_id= 1409863614362787857 - tweet: lack adequ health facil health workforc shortag amp inappropri access health product compound complex procur mechan characterist typifi major sid healthcar access often fragment amp poor qualiti\n",
      "tweet_id= 1427987381609713669 - tweet: drtedro pahowho immedi need ensur sustain humanitarian access amp continu health servic across countri afghanistan focu ensur women amp girl access femal healthworkersdrtedro\n",
      "tweet_id= 1447266150233124865 - tweet: close 1 billion peopl mentalhealth disord despit magnitud mental ill health rel peopl around access qualiti mental health servic hear helen speak truth access mental health care essenti\n",
      "tweet_id= 1410605147340763137 - tweet: 2020 alon 2 billion peopl lack access safe water 36 billion peopl lack access basic toilet 23 billion peopl lack access basic hygien facil find stat mean us\n",
      "tweet_id= 1446475342659653633 - tweet: rt whoafghanistan access health afghan fall reach winter approach health cluster p\n",
      "tweet_id= 1410999807418814466 - tweet: part commit generationequ forum pledg improv access qualiti amp rightsbas famili plan support countri increas adolesc access amp use contracept dissemin updat guidelin safe abort\n",
      "tweet_id= 1430183353638998018 - tweet: facilit effect vaccin deliveri person disabl health provid provid access covid19 vaccin info amp vaccin process ensur access vaccin site vaccinequ\n",
      "tweet_id= 1431861219573575680 - tweet: much easier learn languag provid access knowledg divers nation local languag part solut achiev healthforal visit openwho access free cours 55 languag\n",
      "tweet_id= 1442534948808888325 - tweet: whoacademi offer learner streamlin access who full breadth hundr elearn programm current spread 20 digit learn platform well access high qualiti learn programm develop other\n",
      "tweet_id= 1438927230072152065 - tweet: rt drtedro half popul lebanon current live poverti limit access health care health gain accumulat\n",
      "tweet_id= 1443475536941142018 - tweet: rt unnewscentr health situat afghanistan deterior said reveal decreas access healthcar ju\n",
      "tweet_id= 1442774360905895938 - tweet: today internationalsafeabortionday access safe abort protect health promot human right women girl\n",
      "tweet_id= 1412153587707236375 - tweet: also commit invest evid base sexual amp reproduct health right incl deliv comprehens sexual educ outsid school set improv access qualiti amp rightsbas famili plan access amp use contracept actforequ\n",
      "tweet_id= 1441450992558432257 - tweet: progress provis tuberculosi prevent treatment lag behind 1 5 30 million peopl target access 2022 reach let expand access tb prevent treatment stop infect develop diseas let endtb\n",
      "tweet_id= 1428225478427557888 - tweet: rt drtedro immedi need ensur sustain humanitarian access continu health servic across afghanistan wi\n",
      "tweet_id= 1422999094042169344 - tweet: rt drtedro amp partner 1 provid access lifesav health servic worstaffect crisi 2 support\n",
      "tweet_id= 1423913992712380418 - tweet: uzbekistan state health insur fund cover primaryhealthcar servic peopl access vital health care without fall poverti whoimpact uhcpartnership\n",
      "\n",
      " countries support\n",
      "======================\n",
      "Top 20 results out of 535 for the searched query:\n",
      "\n",
      "tweet_id= 1447624956615987206 - tweet: rt drtedro also need advanc effort support countri integr mentalhealth support school commun\n",
      "tweet_id= 1445433891297406981 - tweet: provid support deliv essenti packag highqual matern newborn servic technic guidanc support countri around\n",
      "tweet_id= 1419540893078626306 - tweet: drtedro open countri bahrain offic reflect who commit serv countri incom level provid support tailor need countri place global health architecturedrtedro healthforal whoimpact\n",
      "tweet_id= 1447421286871977987 - tweet: rt alissonbeck support fifacom worldmentalhealthday let support feel depress reachout\n",
      "tweet_id= 1421113125395484674 - tweet: drtedro 152 countri offic around world central support countri strengthen health system improv health populationsdrtedro whoimpact\n",
      "tweet_id= 1422849022381920260 - tweet: worldbreastfeedingweek hospit support mother breastfe promot infant formula bottl teat support breastfeed\n",
      "tweet_id= 1415647910138687488 - tweet: drtedro jensspahn actacceler committe call countri support who call vaccin least 10 popul everi countri end septemberdrtedro covid19 vaccinequ\n",
      "tweet_id= 1422642008372490244 - tweet: find sweden address declin breastfeed rate use data support polici protect promot support\n",
      "tweet_id= 1423570298599788551 - tweet: hospit support mother breastfe refer commun resourc breastfeed support work commun improv breastfeed servic let support breastfeed mother worldbreastfeedingweek\n",
      "tweet_id= 1435567734503391233 - tweet: rt drtedro remain commit work africanunion end covid19 pandem support african countri\n",
      "tweet_id= 1441375678788681728 - tweet: next year 13 agenc global action plan aim doubl number particip countri roll monitor framework increas support continu strengthen primari health care countri\n",
      "tweet_id= 1423557253802643458 - tweet: hospit support mother breastfe counsel use risk feed bottl teat pacifi support breastfeed mum worldbreastfeedingweek\n",
      "tweet_id= 1419543445505560579 - tweet: drtedro mohbahrain also seek bahrain support control covid19 pandem region global who target support everi countri vaccin least 10 popul end septemb 40 end 2021 70 middl 2022drtedro\n",
      "tweet_id= 1421115705433464836 - tweet: drtedro support countri suppli oxygen guidanc help countri better detect variant continu work daili global network expert understand delta variant spread readilydrtedro covid19\n",
      "tweet_id= 1433417176036175879 - tweet: given challeng dementia carer face support provid includ access inform train servic social financi support detail\n",
      "tweet_id= 1423312801511129094 - tweet: hospit support mother breastfe help know babi hungri limit breastfeed time support breastfeed worldbreastfeedingweek\n",
      "tweet_id= 1422948068614410252 - tweet: hospit support mother breastfe help common breastfeed problem support breastfeed mum anytim anywher worldbreastfeedingweek\n",
      "tweet_id= 1438816241737736193 - tweet: rt drtedro who target support everi countri vaccin least 40 popul end 2021 70\n",
      "tweet_id= 1419737728955060225 - tweet: rt whoemro bahrain steadfast support who work amp shown strong commit multilater countri offic\n",
      "tweet_id= 1409626488324857857 - tweet: support sid polit technic evid financi support better understand address effect climatechang health launch special initi 2018\n",
      "\n",
      " people care\n",
      "======================\n",
      "Top 20 results out of 433 for the searched query:\n",
      "\n",
      "tweet_id= 1433411723046776840 - tweet: support need peopl dementia primari health care specialist care communitybas servic rehabilit longterm care palli care info\n",
      "tweet_id= 1440294899740381194 - tweet: dementia inevit consequ age support need peopl w dementia primari health care specialist care communitybas servic rehabilit longterm care palli care worldalzheimersday\n",
      "tweet_id= 1440310329838170118 - tweet: carer famili peopl live dementia 6 practic tip reach support take care ensur continu care respond chang flexibl commun effect\n",
      "tweet_id= 1447273648318988291 - tweet: seriou gap mentalhealth care result chronic underinvest worldmentalhealthday call invest mental health care sebastian say access mental health care essenti cope covid19 pandem\n",
      "tweet_id= 1446862614139740165 - tweet: today world hospic palli care day 1 peopl need palliativecar receiv let leav one behind make palli care access amp afford here\n",
      "tweet_id= 1420766147964588034 - tweet: countri caribbean improv avail qualiti care critic care patient train empow health worker whoimpact uhcpartnership\n",
      "tweet_id= 1423231583830564868 - tweet: one harm health care yet everi minut 5 patient die unsaf care global patient safeti action plan 20212030 provid strateg direct elimin avoid harm health care amp improv patientsafeti\n",
      "tweet_id= 1446746020445102080 - tweet: today world hospic palli care day dyk 55 million peopl around need palliativecar 12 global need met access palli care servic human right health\n",
      "tweet_id= 1418240095908401156 - tweet: worldbrainday know although current cure dementia support caregiv improv live peopl care\n",
      "tweet_id= 1438859468268257280 - tweet: worldpatientsafetyday take care babi pregnanc attend appoint provid full medic histori ask question follow health care team recommend\n",
      "tweet_id= 1426617942985347075 - tweet: stand solidar peopl of#haiti andpahowhoar work assess health need support emerg medic care\n",
      "tweet_id= 1436228161524613121 - tweet: risk suicid peopl previous tri take life peopl depressionalcoholdrug problem peopl suffer emot distresschron painil peopl experienc violenceabuseoth trauma peopl social isol\n",
      "tweet_id= 1440287735059607565 - tweet: today worldalzheimersday time accur diagnosi provid better care peopl dementia yet mani peopl around world dont access diagnost servic data dementia\n",
      "tweet_id= 1446899209588854788 - tweet: world hospic palli care day train amp support healthwork palliativecar need scale access qualiti servic palli care improv live\n",
      "tweet_id= 1446826919257837568 - tweet: palliativecar everyon babi children seriou ill also benefit palli care care aim reliev suffer whether caus cancer chronic pain neurolog disord\n",
      "tweet_id= 1442499011919835140 - tweet: iarcwho drtedro 2021 design year health care worker recogn invalu contribut health care worker make live worlddrtedro whoacademi\n",
      "tweet_id= 1447422126076669957 - tweet: rt pahowho mani barrier afford access prevent peopl access mental health care ne\n",
      "tweet_id= 1422570918585503747 - tweet: 2021 588 incid attack health care record far 14 countri emerg caus 114 death 278 injuri health care worker patient notatarget\n",
      "tweet_id= 1433380005065609219 - tweet: dementia current 7th lead caus death affect 55 million peopl worldwid here care dementia\n",
      "tweet_id= 1446957241379299338 - tweet: today worldmentalhealthday covid19 major impact peopl mentalhealth health worker student peopl live alon amp preexist condit particularli affect letstalk today care amp support askwho\n"
     ]
    }
   ],
   "source": [
    "our_queries=['covid19 vaccine', 'pandemic', 'health access', 'countries support', 'people care']\n",
    "\n",
    "queries_df = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    query = our_queries[i]\n",
    "    ranked_docs, doc_scores_df, docs_id_df = search_tf_idf(query, index)\n",
    "    \n",
    "    hashtag_found = [0]*len(docs_id_df)\n",
    "    \n",
    "    for word in query.split():\n",
    "        for doc in range(len(docs_id_df)):\n",
    "            for h in hashtags[docs_id_df[doc]]:\n",
    "                #print(h)\n",
    "                if h == word:\n",
    "                    hashtag_found[doc] = 1 #as it is binary\n",
    "    if i == 0:\n",
    "        queries_df = pd.DataFrame()\n",
    "        queries_df['doc_id'] = docs_id_df\n",
    "        queries_df['q_id'] = i\n",
    "        queries_df['predicted_relevance'] = doc_scores_df\n",
    "        queries_df['y_true'] = hashtag_found\n",
    "    else:\n",
    "        #print(\"here\")\n",
    "        query_df = pd.DataFrame()\n",
    "        query_df['doc_id'] = docs_id_df\n",
    "        query_df['q_id'] = i\n",
    "        query_df['predicted_relevance'] = doc_scores_df\n",
    "        query_df['y_true'] = hashtag_found\n",
    "        queries_df = pd.concat([queries_df, query_df], ignore_index=True)\n",
    "    \n",
    "    top = 20\n",
    "    print(\"\\n\", query)\n",
    "    print(\"======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "    for d_id in ranked_docs[:top]:\n",
    "        print(\"tweet_id= {} - tweet: {}\".format(d_id, title_index[d_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>q_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1448208458604584960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1448163383493136385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1448049654923468803</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1448031156348362754</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1448031115428696064</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>1409489891797446656</td>\n",
       "      <td>4</td>\n",
       "      <td>0.834446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>1408450795570335747</td>\n",
       "      <td>4</td>\n",
       "      <td>1.152974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>1408450214139240452</td>\n",
       "      <td>4</td>\n",
       "      <td>1.274653</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>1408449858365710339</td>\n",
       "      <td>4</td>\n",
       "      <td>0.956125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>1408449282731122699</td>\n",
       "      <td>4</td>\n",
       "      <td>1.209217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2771 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc_id  q_id  predicted_relevance  y_true\n",
       "0     1448208458604584960     0             0.201941       1\n",
       "1     1448163383493136385     0             0.177700       0\n",
       "2     1448049654923468803     0             0.849441       1\n",
       "3     1448031156348362754     0             0.285646       1\n",
       "4     1448031115428696064     0             0.264472       1\n",
       "...                   ...   ...                  ...     ...\n",
       "2766  1409489891797446656     4             0.834446       0\n",
       "2767  1408450795570335747     4             1.152974       0\n",
       "2768  1408450214139240452     4             1.274653       0\n",
       "2769  1408449858365710339     4             0.956125       0\n",
       "2770  1408449282731122699     4             1.209217       0\n",
       "\n",
       "[2771 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The favorite_count provides the number of times the tweet has been favorited.\n",
    "#The retweet_count is the count of the retweets of the source tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet id: 1448215930178310144\n",
      "Terms: ['intern', 'day', 'disast', 'risk', 'reduct', 'openwho', 'launch', 'multiti', 'core', 'curriculum', 'help', 'equip', 'compet', 'need', 'work', 'within', 'public', 'health', 'emerg', 'respons', 'start', 'learn', 'today', 'amp', 'ready4respons']\n",
      "Title: intern day disast risk reduct openwho launch multiti core curriculum help equip compet need work within public health emerg respons start learn today amp ready4respons\n",
      "Number of retweets: 16\n",
      "Number of favorites: 52\n"
     ]
    }
   ],
   "source": [
    "tweet_id = int(tweets_data[0]['id'])\n",
    "terms =  tweets_data[0]['full_text'] #page_title + page_text\n",
    "title = \" \".join(tweets_data[0]['full_text'])\n",
    "title_index[0]=title\n",
    "num_retweets = tweets_data[0]['retweet_count'] #num retweets\n",
    "favourite_count = tweets_data[0]['favorite_count'] #num of favourite\n",
    "\n",
    "print('Tweet id:', tweet_id)\n",
    "print('Terms:', terms)\n",
    "print('Title:', title)\n",
    "print('Number of retweets:', num_retweets)\n",
    "print('Number of favorites:', favourite_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_our_score(tweets_data, num_tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets\n",
    "    num_documents -- total number of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    popularity - parameter of popularity depending on number of retweets + favourites\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in tweets (tweets in the same order as in the main index)\n",
    "    df = defaultdict(int)  #tweet frequencies of terms in the corpus\n",
    "    title_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "    popularity = []\n",
    "\n",
    "    for tweet in tweets_data:\n",
    "        \n",
    "        tweet_id = int(tweets_data[tweet]['id'])\n",
    "        terms =  tweets_data[tweet]['full_text']#page_title + page_text\n",
    "        title = \" \".join(tweets_data[tweet]['full_text'])\n",
    "        title_index[tweet_id]=title\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] +=1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_tweets/df[term])), 4)\n",
    "            \n",
    "        num_retweets = tweets_data[tweet]['retweet_count'] #num retweets\n",
    "        favourite_count = tweets_data[tweet]['favorite_count'] #num of favourite\n",
    "        \n",
    "        popularity.append(num_retweets + favourite_count)\n",
    "        \n",
    "        \n",
    "    popularity = (popularity)/np.linalg.norm(popularity) * 100\n",
    "\n",
    "    \n",
    "    return index, tf, df, idf, title_index, popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 55.79 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_tweets = len(tweets_data)\n",
    "index, tf, df, idf, title_index, popularity = create_index_our_score(tweets_data, num_tweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_our_score(terms, docs, index, idf, tf, title_index, popularity):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term] * popularity[doc_index] # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each tweet \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores_df = [x[0] for x in doc_scores]\n",
    "    docs_id_df = [x[1] for x in doc_scores]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    result_doc_scores = [x[0] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    \n",
    "    return result_docs, doc_scores_df, docs_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_our_score(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of tweets that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query, hashtags = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs, doc_scores_df, docs_id_df = rank_documents_our_score(query, docs, index, idf, tf, title_index, popularity)\n",
    "    \n",
    "    return ranked_docs, doc_scores_df, docs_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " covid19 vaccine\n",
      "======================\n",
      "Top 20 results out of 887 for the searched query:\n",
      "\n",
      "tweet_id= 1425405666580172803 - tweet: rt live qampa vaccinequ dr mariangelasimao amp dr bruce aylward askwho question covid19 vaccin equiti\n",
      "tweet_id= 1427991963932975108 - tweet: drtedro actacceler urg jampj urgent priorit distribut vaccin africa consid suppli rich countri alreadi suffici accessdrtedro vaccinequ\n",
      "tweet_id= 1433070724781154310 - tweet: drtedro covid19 pandem defin crisi time taught world mani pain lessonsdrtedro\n",
      "tweet_id= 1435598519595843585 - tweet: rt whoafro join tomorrow whoafro live press brief covid19 pandem genom sequenc covid19 variant africa dr\n",
      "tweet_id= 1438528587783495686 - tweet: rt whoafro whoafro covid19 press brief pandem amp impact covax vaccin suppli challeng speaker includ dr moeti\n",
      "tweet_id= 1418205159188955137 - tweet: covid19 vaccin inequ undermin econom recoveri undp blavatnikschool dashboard vaccinequ find lowincom countri would add 38b gdp forecast vaccin rate highincom countri\n",
      "tweet_id= 1425446081169608711 - tweet: drtedro contrast sever effect vaccin covid19 yet case death continu risedrtedro\n",
      "tweet_id= 1421116365138796551 - tweet: drtedro last month announc set technolog transfer hub mrna vaccin southafrica part effort scale product vaccinesdrtedro vaccinequ\n",
      "tweet_id= 1443517908072730624 - tweet: rt whoafro live whoafro media brief covid19 pandem amp global 10 covid19 vaccin mileston reach countri\n",
      "tweet_id= 1442132750191013888 - tweet: step school take keep student staff safe covid19 vaccin famili commun keep safe school reopen dr mvankerkhov explain sciencein5 week\n",
      "tweet_id= 1435321728197177347 - tweet: blood clot receiv covid19 astrazeneca janssen vaccin rare get vaccin soon turn\n",
      "tweet_id= 1446027051618775043 - tweet: want learn first ever malaria vaccin use join twitterspac chat katelobrien palonsomalaria today 1100 cest askwho\n",
      "tweet_id= 1426206173225787392 - tweet: rt whowpro encourag love one get vaccin covid19 help regist get vaccin site vaccin\n",
      "tweet_id= 1417654879246503942 - tweet: olymp drtedro tokyo2020 paralymp g20org actacceler may tokyo2020 game moment unit ignit solidar determin need end covid19 pandem togeth vaccin 70 popul everi countri middl next yeardrtedro agoal4al\n",
      "tweet_id= 1426172125640003586 - tweet: drtedro far 75 covid19 vaccin avail administ 10 countri lowincom countri receiv 1 nowher near enough fulli vaccin healthwork risk popul vaccinequ\n",
      "tweet_id= 1437766161043709960 - tweet: rt drtedro pleas whoeurop almost 90 member state reach 10 vaccin target amp 60 alreadi reach\n",
      "tweet_id= 1417641674734456834 - tweet: olymp drtedro anyon think covid19 pandem live live fool paradis vaccin power essenti tool world use welldrtedro tokyo2020 agoal4al\n",
      "tweet_id= 1427219695204188169 - tweet: rt drtedro achiev vaccinequ amp acceler vaccin 10 popul everywher septemb need 1 countri share\n",
      "tweet_id= 1416433609091653633 - tweet: covid19 vaccin covid19 vaccin 10 countri rest vaccinequ end pandem togeth worldemojiday\n",
      "tweet_id= 1413076754575642634 - tweet: rt whoafro live whoafro press brief covid19 third wave new variant amp vaccin rollout africa speaker includ\n",
      "\n",
      " pandemic\n",
      "======================\n",
      "Top 20 results out of 233 for the searched query:\n",
      "\n",
      "tweet_id= 1412768782934806529 - tweet: well look acceler end covid19 pandem give children support need must top prioriti come must help come term experienc chanc hope futuredrtedro\n",
      "tweet_id= 1430105912795148304 - tweet: 2 urg invest local product vaccin health product part invest pandem prepared responsedrtedro rc71afro\n",
      "tweet_id= 1425874774534860805 - tweet: sinc start covid19 pandem work member state scientif commun better understand pandem began better prepar next one\n",
      "tweet_id= 1437741809753538561 - tweet: drmikeryan explain criteria call pandem public health emerg intern concern\n",
      "tweet_id= 1442034337403256835 - tweet: contracept famili plan inform servic lifesav import time sexual activ stop covid19 pandem worldcontraceptionday\n",
      "tweet_id= 1414596227698774025 - tweet: drtedro hose part might reduc flame one area smoulder anywher spark eventu travel amp grow roar furnac battl togeth put covid19 pandem inferno everywheredrtedro\n",
      "tweet_id= 1442554614226771968 - tweet: invest health system best way prepar futur pandem success requir unpreced coordin actor key player whoacademi essenti platform dissemin learn presid emmanuelmacron\n",
      "tweet_id= 1410606368466235395 - tweet: almost 18 month world health care worker forefront covid19 pandem remind us almost daili incred peopl incred job incred circumstancesdrtedro generationequ\n",
      "tweet_id= 1420487824957231108 - tweet: idea polit declar pandem prepared respons member state decid move forward initi would pleas lead processdrmikeryan\n",
      "tweet_id= 1410944161201655814 - tweet: mani small island develop state succeed prevent widespread transmiss covid19 commun pandem hit hard way declin revenu tourism affect economi significantlydrtedro\n",
      "tweet_id= 1420486398243745795 - tweet: covid19 pandem reveal two critic area world need far bolster global health secur strengthen sharingdrmikeryan\n",
      "tweet_id= 1415512673928757255 - tweet: due covid19 pandem 23 million children miss basic vaccin routin immun servic 2020 increas 37 million highest number sinc 2009 data amp unicef\n",
      "tweet_id= 1425874780901752834 - tweet: search covid19 viru origin isnt amp shouldnt exercis attribut blame fingerpoint polit pointscor vital know pandem began set exampl establish origin futur animalhuman spillov event\n",
      "tweet_id= 1415647098918383619 - tweet: drtedro jensspahn actacceler committe express concern covid19 pandem mischaracter come end nowher near finisheddrtedro\n",
      "tweet_id= 1435611635700445184 - tweet: drtedro mening outbreak drc anoth remind although covid19 pandem continu domin headlin far health threat respond around worlddrtedro\n",
      "tweet_id= 1433050831566172162 - tweet: live inaugur hub pandem amp epidem intellig drtedro amp chancellor merkel\n",
      "tweet_id= 1408028717763416065 - tweet: today day 1 selfcar month throughout month well share tip help protect health covid19 pandem beyond\n",
      "tweet_id= 1412875962795143169 - tweet: rt drtedro world peril point covid19 pandem pass tragic mileston 4 million record dea\n",
      "tweet_id= 1432785919434792969 - tweet: 1 septemb thank germani chancellor angela merkel strong support inaugur new hub pandem epidem intellig berlin follow live event\n",
      "tweet_id= 1430554094888960005 - tweet: recent g20org establish highlevel independ panel financ global common pandem prepared responsedrtedro\n",
      "\n",
      " health access\n",
      "======================\n",
      "Top 20 results out of 683 for the searched query:\n",
      "\n",
      "tweet_id= 1418159921955225601 - tweet: worldbrainday multipl sclerosi affect 28 million peopl everi 5 minut someon diagnos treatment slow diseas progress improv qualiti life yet access treatment limit global\n",
      "tweet_id= 1433075618208522248 - tweet: drtedro chikwei schwartlanderb 2015 address wha68 critic moment global health 2017 first week directorgener honour attend g20 summit hamburg put strong emphasi health amp emerg preparednessdrtedro\n",
      "tweet_id= 1435564343681183746 - tweet: right health inform help peopl disabl get access right health servic let build disabilityinclus health sector amp leav one behind healthforal\n",
      "tweet_id= 1410607257184731148 - tweet: includ valu reward women health care workforc guarante workplac free discrimin violenc sexual exploit abusedrtedro generationequ\n",
      "tweet_id= 1439970387799027718 - tweet: rt drtedro unga high level week start today urg leader 1 guarante vaccinequ amp equit access covid19\n",
      "tweet_id= 1409802786146037766 - tweet: seen progress genderequ 26 year sinc beijingplatform recognit women leadership healthcar reduct matern mortal amp improv access famili plan work need\n",
      "tweet_id= 1443489829598203904 - tweet: also call technolog transfer allow manufactur biosimilar version around patient may need treatment access\n",
      "tweet_id= 1442847823700152321 - tweet: drtedro un receiv inform commiss ident amp locat ensur access servic need incl medic amp psychosoci support amp assist educ childrendrtedro\n",
      "tweet_id= 1409863614362787857 - tweet: lack adequ health facil health workforc shortag amp inappropri access health product compound complex procur mechan characterist typifi major sid healthcar access often fragment amp poor qualiti\n",
      "tweet_id= 1441373266875064320 - tweet: rt drtedro pleasur join princ harri meghan duke amp duchess sussex amp group leader health polit amp\n",
      "tweet_id= 1443550360824696840 - tweet: 2 year ago world leader commit make univers health coverag realiti covid19 crisi underscor need deliv promis urgent everyon everywher access qualiti amp afford health care unga\n",
      "tweet_id= 1428225478427557888 - tweet: rt drtedro immedi need ensur sustain humanitarian access continu health servic across afghanistan wi\n",
      "tweet_id= 1410605147340763137 - tweet: 2020 alon 2 billion peopl lack access safe water 36 billion peopl lack access basic toilet 23 billion peopl lack access basic hygien facil find stat mean us\n",
      "tweet_id= 1437865490806976515 - tweet: leader stress worst pandem last hundr year covid19 end unless genuin global cooper vaccin suppli access vaccinequ\n",
      "tweet_id= 1419541721126424589 - tweet: drtedro who vision world peopl access health servic need without face financi hardship know bahrain share visiondrtedro healthforal\n",
      "tweet_id= 1428377040214974471 - tweet: take climateact call amp join thehumanrac safeguard health amp world worldhumanitarianday\n",
      "tweet_id= 1425874790557102082 - tweet: commit follow scienc call govern put differ asid work togeth provid data access requir next seri studi commenc soon possibl\n",
      "tweet_id= 1427987381609713669 - tweet: drtedro pahowho immedi need ensur sustain humanitarian access amp continu health servic across countri afghanistan focu ensur women amp girl access femal healthworkersdrtedro\n",
      "tweet_id= 1426509340744327170 - tweet: rt drtedro countri high coverag see decoupl covid19 case amp death wherea countri cant access vaccin\n",
      "tweet_id= 1426204451879231495 - tweet: rt whoeurop your area affect wildfir monitor amp follow recommend local health emerg author thro\n",
      "\n",
      " countries support\n",
      "======================\n",
      "Top 20 results out of 535 for the searched query:\n",
      "\n",
      "tweet_id= 1419540893078626306 - tweet: drtedro open countri bahrain offic reflect who commit serv countri incom level provid support tailor need countri place global health architecturedrtedro healthforal whoimpact\n",
      "tweet_id= 1408449282731122699 - tweet: drtedro emerg medic team emt group health profession includ doctor nurs paramed support worker logistician provid care patient affect emergencydrtedro\n",
      "tweet_id= 1432749720359821321 - tweet: support health emerg prepared respons amp top donor conting fund emerg contribut us68 million flexibl fund sinc 2015 support who respons 120 emerg\n",
      "tweet_id= 1412768782934806529 - tweet: well look acceler end covid19 pandem give children support need must top prioriti come must help come term experienc chanc hope futuredrtedro\n",
      "tweet_id= 1422909337480089601 - tweet: drtedro far 4 billion covid19 vaccin dose administ global 80 gone high uppermiddl incom countri even though account less half world populationdrtedro\n",
      "tweet_id= 1435613797104955408 - tweet: drtedro almost 90 highincom countri reach 10 target 70 reach 40 drtedro vaccinequ\n",
      "tweet_id= 1408450061059690497 - tweet: drtedro greec team norway germani deploy follow fire refuge camp island lesvo support covid19 respons togeth essenti health servicesdrtedro\n",
      "tweet_id= 1415644806068514818 - tweet: drtedro jensspahn may add vielen dank jensspahn person support past year especi past 18 month incred support german govern peopl global healthdrtedro actogeth\n",
      "tweet_id= 1440968119753875457 - tweet: drtedro whoemro object countri lebanon afghnaistan see health challeng commun face amp engag senior polit leader assess best support health system sever straindrtedro\n",
      "tweet_id= 1423312801511129094 - tweet: hospit support mother breastfe help know babi hungri limit breastfeed time support breastfeed worldbreastfeedingweek\n",
      "tweet_id= 1442847670201159687 - tweet: drtedro un provid servic support victim survivor central concern yet awar ident victim survivorsdrtedro\n",
      "tweet_id= 1419542302188572672 - tweet: drtedro mohbahrain 2we urg continu comprehens approach protect peopl covid19 pandem long way everi countri must continu tailor consist approach public health measur vaccinationdrtedro\n",
      "tweet_id= 1412499521632878593 - tweet: increas access afford lifesav product call manufactur reduc price make suppli avail low middleincom countri especi covid19 surg\n",
      "tweet_id= 1422910210704826377 - tweet: drtedro meanwhil lowincom countri abl administ 15 everi 100 peopl due lack suppli need urgent revers major vaccin go highincom countri major go lowincom countriesdrtedro vaccinequ\n",
      "tweet_id= 1421115477661822980 - tweet: drtedro test rate lowincom countri less 2 highincom countri leav blind understand diseas changingdrtedro covid19\n",
      "tweet_id= 1422948068614410252 - tweet: hospit support mother breastfe help common breastfeed problem support breastfeed mum anytim anywher worldbreastfeedingweek\n",
      "tweet_id= 1410868057237512193 - tweet: madagascar provid support maintain routin child vaccin covid19 scale vaccin includ remot area detail whoimpact\n",
      "tweet_id= 1427986589842608130 - tweet: drtedro colleagu pahowho help send emerg medic team amp suppli also ground support author assess damag health facil four destroy amp 20 damageddrtedro\n",
      "tweet_id= 1443907932014403589 - tweet: world miss target set vaccin peopl countri end septemb vaccin equit moral epidemiolog econom necess vaccinequ\n",
      "tweet_id= 1409891374770577411 - tweet: drtedro listen care amp vision health pleas hear outcom statement key element address summit amp concret request secretariat partner countri global health communitydrtedro\n",
      "\n",
      " people care\n",
      "======================\n",
      "Top 20 results out of 433 for the searched query:\n",
      "\n",
      "tweet_id= 1421115869577494532 - tweet: drtedro need patient receiv earli clinic care train protect healthwork oxygen treat serious ill save livesdrtedro covid19\n",
      "tweet_id= 1417653459675979779 - tweet: olymp drtedro tokyo2020 paralymp g20org actacceler us govern compani civil societi individu ioc fight infodem depriv peopl lifesav inform peddl deadli liesdrtedro tokyo2020 agoal4al\n",
      "tweet_id= 1423913992712380418 - tweet: uzbekistan state health insur fund cover primaryhealthcar servic peopl access vital health care without fall poverti whoimpact uhcpartnership\n",
      "tweet_id= 1437382169266098179 - tweet: rt ungeneva unit nation abandon peopl afghanistan donor urg fasttrack fund allow humanitarian\n",
      "tweet_id= 1443550360824696840 - tweet: 2 year ago world leader commit make univers health coverag realiti covid19 crisi underscor need deliv promis urgent everyon everywher access qualiti amp afford health care unga\n",
      "tweet_id= 1418159921955225601 - tweet: worldbrainday multipl sclerosi affect 28 million peopl everi 5 minut someon diagnos treatment slow diseas progress improv qualiti life yet access treatment limit global\n",
      "tweet_id= 1410607450063921157 - tweet: global health care sector employ greater percentag women mean action better support women field make immedi differ women workforc wholedrtedro generationequ\n",
      "tweet_id= 1421049468917620740 - tweet: social isol loneli among older peopl widespread exampl 1 3 older peopl lone countri region china europ india latin america unit state internationaldayoffriendship\n",
      "tweet_id= 1438877049825943553 - tweet: improv amp safeti point care healthcar facil manag amp health worker adopt 5 worldpatientsafetyday goal incl strengthen capac amp support health worker safe use medic amp blood transfus\n",
      "tweet_id= 1445779797972439041 - tweet: drtedro power new tool like covid19 vaccin tool vaccin malaria replac reduc need measur includ bednet seek care feverdrtedro endmalaria\n",
      "tweet_id= 1412429818705108998 - tweet: rt drtedro million peopl press border activ war zone northwest syria call renew u\n",
      "tweet_id= 1410200562927288325 - tweet: countri report covid19 death occur hospit peopl test posit mani countri cannot accur measur report caus death due inadequ underresourc health inform system healthdata\n",
      "tweet_id= 1422432394355822616 - tweet: depressionaffect 260 million peopl world around half mentalhealthcondit start age 14 suicidei fourth lead caus death young peopl age 1529 reachout\n",
      "tweet_id= 1442499672686333956 - tweet: whoacademi provid million peopl around rapid access highest qualiti lifelong learn health offer multilingu person learn programm digit inperson blend format\n",
      "tweet_id= 1444355197300531200 - tweet: ageism affect peopl age affect health wellb social system economi time chang help us creat aworld4allag\n",
      "tweet_id= 1410877886081404929 - tweet: syria deliv 40 ambul respond urgent humanitarian need improv health care vulner area camp intern displac peopl amp refuge detail whoimpact\n",
      "tweet_id= 1424990041969995804 - tweet: rt drtedro past week devast fire impact mani peopl live continu busi usual risk pose\n",
      "tweet_id= 1440310329838170118 - tweet: carer famili peopl live dementia 6 practic tip reach support take care ensur continu care respond chang flexibl commun effect\n",
      "tweet_id= 1422570918585503747 - tweet: 2021 588 incid attack health care record far 14 countri emerg caus 114 death 278 injuri health care worker patient notatarget\n",
      "tweet_id= 1435255817037664257 - tweet: 600 million peopl fall ill everi year eat food contamin bacteria virus parasit toxin chemic everyon role play keep food safe\n"
     ]
    }
   ],
   "source": [
    "queries_df = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    query = our_queries[i]\n",
    "    ranked_docs, doc_scores_df, docs_id_df = search_our_score(query, index)\n",
    "    \n",
    "    hashtag_found = [0]*len(docs_id_df)\n",
    "    \n",
    "    for word in query.split():\n",
    "        for doc in range(len(docs_id_df)):\n",
    "            for h in hashtags[docs_id_df[doc]]:\n",
    "                #print(h)\n",
    "                if h == word:\n",
    "                    hashtag_found[doc] = 1 #as it is binary\n",
    "    if i == 0:\n",
    "        queries_df = pd.DataFrame()\n",
    "        queries_df['doc_id'] = docs_id_df\n",
    "        queries_df['q_id'] = i\n",
    "        queries_df['predicted_relevance'] = doc_scores_df\n",
    "        queries_df['y_true'] = hashtag_found\n",
    "    else:\n",
    "        #print(\"here\")\n",
    "        query_df = pd.DataFrame()\n",
    "        query_df['doc_id'] = docs_id_df\n",
    "        query_df['q_id'] = i\n",
    "        query_df['predicted_relevance'] = doc_scores_df\n",
    "        query_df['y_true'] = hashtag_found\n",
    "        queries_df = pd.concat([queries_df, query_df], ignore_index=True)\n",
    "    \n",
    "    top = 20\n",
    "    print(\"\\n\", query)\n",
    "    print(\"======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "    for d_id in ranked_docs[:top]:\n",
    "        print(\"tweet_id= {} - tweet: {}\".format(d_id, title_index[d_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creem Word2Vec Model\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "for key in range(length):\n",
    "    all_words+=tweets_data[key][\"full_text\"]\n",
    "all_words_unique = np.unique(all_words)\n",
    "all_words = [all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets=[]\n",
    "for key in range(length):\n",
    "    all_tweets.append(tweets_data[key][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pau\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences = all_words, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=[]\n",
    "for query in our_queries:\n",
    "    query,_ = build_terms(query)\n",
    "    queries.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweet_query(tweet, query):\n",
    "    word_query = []\n",
    "    for w in all_words_unique:\n",
    "        a = []\n",
    "        for q in query:\n",
    "            a.append(np.dot(model[q], model[w]))\n",
    "        word_query.append(np.mean(a))\n",
    "\n",
    "    word_tweet = []\n",
    "    for w in all_words_unique:\n",
    "        b = []\n",
    "        for t in tweet:\n",
    "            b.append(np.dot(model[t], model[w]))\n",
    "        word_tweet.append(np.mean(b))\n",
    "    return np.dot(word_query,word_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pau\\AppData\\Local\\Temp/ipykernel_9576/2578313831.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  a.append(np.dot(model[q], model[w]))\n",
      "C:\\Users\\Pau\\AppData\\Local\\Temp/ipykernel_9576/2578313831.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  b.append(np.dot(model[t], model[w]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101168.8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet=all_tweets[0]\n",
    "my_query = queries[0]\n",
    "tweet_query(my_tweet, my_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scores(tweets_data,all_tweets, query, k_top):\n",
    "    tweet_dict={}\n",
    "    for i in range(len(tweets_data))[0:200]:\n",
    "        tweet=all_tweets[i]\n",
    "        tweet_id=tweets_data[i][\"id\"]\n",
    "        tweet_dict[tweet_id]=tweet_query(tweet, query)\n",
    "        \n",
    "    k = Counter(tweet_dict)\n",
    "    high = k.most_common(k_top)\n",
    "    return high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY:  covid19 vaccin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pau\\AppData\\Local\\Temp/ipykernel_9576/2578313831.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  a.append(np.dot(model[q], model[w]))\n",
      "C:\\Users\\Pau\\AppData\\Local\\Temp/ipykernel_9576/2578313831.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  b.append(np.dot(model[t], model[w]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1447226874262872067, 149004.4), (1446111322739863577, 148547.84), (1446113678776889344, 140102.89), (1447226893783248898, 138054.45), (1447624956615987206, 136688.84), (1446582377711554569, 133625.47), (1447972516161490944, 133145.19), (1446481486346170377, 131655.25), (1447421375321358341, 129712.27), (1446424624141684738, 128736.46)]\n",
      "\n",
      "QUERY:  pandem\n",
      "[(1447226874262872067, 135361.7), (1446111322739863577, 134946.94), (1446113678776889344, 127275.2), (1447226893783248898, 125414.31), (1447624956615987206, 124173.734), (1446582377711554569, 121390.84), (1447972516161490944, 120954.54), (1446481486346170377, 119601.016), (1447421375321358341, 117835.94), (1446424624141684738, 116949.47)]\n",
      "\n",
      "QUERY:  health access\n",
      "[(1447226874262872067, 133600.44), (1446111322739863577, 133191.06), (1446113678776889344, 125619.15), (1447226893783248898, 123782.48), (1447624956615987206, 122558.04), (1446582377711554569, 119811.36), (1447972516161490944, 119380.734), (1446481486346170377, 118044.81), (1447421375321358341, 116302.71), (1446424624141684738, 115427.766)]\n",
      "\n",
      "QUERY:  countri support\n",
      "[(1447226874262872067, 138113.78), (1446111322739863577, 137690.58), (1446113678776889344, 129862.875), (1447226893783248898, 127964.15), (1447624956615987206, 126698.35), (1446582377711554569, 123858.89), (1447972516161490944, 123413.7), (1446481486346170377, 122032.66), (1447421375321358341, 120231.7), (1446424624141684738, 119327.2)]\n",
      "\n",
      "QUERY:  peopl care\n",
      "[(1447226874262872067, 134130.69), (1446111322739863577, 133719.67), (1446113678776889344, 126117.71), (1447226893783248898, 124273.75), (1447624956615987206, 123044.46), (1446582377711554569, 120286.88), (1447972516161490944, 119854.54), (1446481486346170377, 118513.33), (1447421375321358341, 116764.3), (1446424624141684738, 115885.89)]\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(\"\\nQUERY: \", \" \".join(query))\n",
    "    high = find_scores(tweets_data,all_tweets, query, 10)\n",
    "    print(high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
